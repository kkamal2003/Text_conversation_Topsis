{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-26T14:02:25.900047Z","iopub.execute_input":"2024-01-26T14:02:25.900524Z","iopub.status.idle":"2024-01-26T14:02:37.985261Z","shell.execute_reply.started":"2024-01-26T14:02:25.900487Z","shell.execute_reply":"2024-01-26T14:02:37.984087Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:02:37.987741Z","iopub.execute_input":"2024-01-26T14:02:37.988123Z","iopub.status.idle":"2024-01-26T14:02:37.994758Z","shell.execute_reply.started":"2024-01-26T14:02:37.988087Z","shell.execute_reply":"2024-01-26T14:02:37.993604Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install rouge-score","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:02:37.996107Z","iopub.execute_input":"2024-01-26T14:02:37.996444Z","iopub.status.idle":"2024-01-26T14:02:50.127150Z","shell.execute_reply.started":"2024-01-26T14:02:37.996413Z","shell.execute_reply":"2024-01-26T14:02:50.125940Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: rouge-score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.24.3)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import csv\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom rouge_score import rouge_scorer\nimport torch\nimport numpy as np\n\n# Define models and their corresponding prompts\nmodels = [\n    {\"name\": \"facebook/blenderbot-400M-distill\", \"prompt\": \"What is your favorite genre of music and why?\"},\n    {\"name\": \"facebook/blenderbot-3B\", \"prompt\": \"Who are some of the most influential musicians or bands in your opinion?\"},\n    {\"name\": \"Pi3141/DialoGPT-medium-elon-2\", \"prompt\": \"How do you discover new music? Do you use any apps or websites?\"},\n    {\"name\": \"ToddGoldfarb/Cadet-Tiny\", \"prompt\": \"How often do you listen to music and where do you listen to it?\"},\n    {\"name\": \"satvikag/chatbot\", \"prompt\": \"Do you prefer to listen to music with lyrics or without lyrics? Why?\"},\n]\n\n# Define generated prompts for each evaluation prompt\ngenerated_prompts = [\n    \"Classical music for its soothing and timeless qualities\",\n    \"Beethoven, Mozart, Bach, Chopin, and Tchaikovsky are influential musicians\",\n    \"I discover new music by using Bing and YouTube.\",\n    \"I listen to music daily, mostly on my phone\",\n    \"I prefer music with lyrics to understand emotions and learn languages.\"\n]\n\n# Initialize tokenizer and model for each model\nfor model_info in models:\n    model_info[\"tokenizer\"] = AutoTokenizer.from_pretrained(model_info[\"name\"])\n    if model_info[\"name\"] in [\"Pi3141/DialoGPT-medium-elon-2\", \"satvikag/chatbot\"]:\n        model_info[\"model\"] = AutoModelForCausalLM.from_pretrained(model_info[\"name\"])\n    else:\n        model_info[\"model\"] = AutoModelForSeq2SeqLM.from_pretrained(model_info[\"name\"])\n\n# Define evaluation metrics\ndef calculate_bleu(reference, candidate):\n    return sentence_bleu([reference.split()], candidate.split())\n\ndef calculate_rouge(reference, candidate):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = scorer.score(reference, candidate)\n    rouge1 = scores['rouge1'].fmeasure\n    rouge2 = scores['rouge2'].fmeasure\n    rougeL = scores['rougeL'].fmeasure\n    return rouge1, rouge2, rougeL\n\ndef calculate_response_length(text):\n    return len(text.split())\n\n# Store responses for each model\nresponses = []\n\n# Generate responses for each prompt using each model\nfor i, model_info in enumerate(models):\n    prompt_text = model_info[\"prompt\"]\n    generated_prompt = generated_prompts[i]\n    responses_for_prompt = {\"prompt\": prompt_text}\n    for model_info in models:\n        tokenizer = model_info[\"tokenizer\"]\n        model = model_info[\"model\"]\n        input_ids = tokenizer(generated_prompt, return_tensors=\"pt\").input_ids\n        output = model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=0.9)\n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        responses_for_prompt[model_info[\"name\"]] = response\n    responses.append(responses_for_prompt)\n\n# Initialize dictionaries to store total metrics and count\ntotal_metrics = {model[\"name\"]: {\"BLEU\": 0, \"ROUGE-1\": 0, \"ROUGE-2\": 0, \"ROUGE-L\": 0, \"Response Length\": 0} for model in models}\ncount = {model[\"name\"]: 0 for model in models}\n\n# Compare responses using evaluation metrics and calculate total metrics\nfor response in responses:\n    for model_info in models:\n        model_name = model_info[\"name\"]\n        response_text = response[model_name]\n        # Get the generated prompt for evaluation\n        generated_prompt = response[\"prompt\"]\n        # Evaluate response using BLEU score\n        bleu_score = calculate_bleu(generated_prompt, response_text)\n        # Evaluate response using ROUGE score\n        rouge1, rouge2, rougeL = calculate_rouge(generated_prompt, response_text)\n        # Evaluate response using response length\n        response_length = calculate_response_length(response_text)\n        # Update total metrics and count\n        total_metrics[model_name][\"BLEU\"] += bleu_score\n        total_metrics[model_name][\"ROUGE-1\"] += rouge1\n        total_metrics[model_name][\"ROUGE-2\"] += rouge2\n        total_metrics[model_name][\"ROUGE-L\"] += rougeL\n        total_metrics[model_name][\"Response Length\"] += response_length\n        count[model_name] += 1\n\n# Calculate average metrics\naverage_metrics = {}\nfor model_name in total_metrics:\n    average_metrics[model_name] = {metric: total_metrics[model_name][metric] / count[model_name] for metric in total_metrics[model_name]}\n\n# Save average metrics to a CSV file\nwith open(\"evaluation_results.csv\", \"w\", newline=\"\") as csvfile:\n    fieldnames = [\"Model\", \"BLEU\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"Response Length\"]\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n    writer.writeheader()\n    for model_name, metrics in average_metrics.items():\n        row = {\"Model\": model_name}\n        row.update(metrics)\n        writer.writerow(row)\n\nprint(\"Evaluation results saved to evaluation_results.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:02:50.132008Z","iopub.execute_input":"2024-01-26T14:02:50.132418Z","iopub.status.idle":"2024-01-26T14:07:00.789383Z","shell.execute_reply.started":"2024-01-26T14:02:50.132368Z","shell.execute_reply":"2024-01-26T14:07:00.788232Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Evaluation results saved to evaluation_results.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install Topsis-Kamal-102103259","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:07:00.792065Z","iopub.execute_input":"2024-01-26T14:07:00.793106Z","iopub.status.idle":"2024-01-26T14:07:13.270883Z","shell.execute_reply.started":"2024-01-26T14:07:00.793073Z","shell.execute_reply":"2024-01-26T14:07:13.269247Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: Topsis-Kamal-102103259 in /opt/conda/lib/python3.10/site-packages (1.0.14)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from Topsis-Kamal-102103259) (2.0.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from Topsis-Kamal-102103259) (1.24.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->Topsis-Kamal-102103259) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->Topsis-Kamal-102103259) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->Topsis-Kamal-102103259) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->Topsis-Kamal-102103259) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!python -m Topsis_Kamal_102103259 evaluation_results.csv \"1,1,1,1,0.5\" \"+,+,+,+,-\" output.csv","metadata":{"execution":{"iopub.status.busy":"2024-01-26T14:08:29.392002Z","iopub.execute_input":"2024-01-26T14:08:29.392496Z","iopub.status.idle":"2024-01-26T14:08:31.420564Z","shell.execute_reply.started":"2024-01-26T14:08:29.392454Z","shell.execute_reply":"2024-01-26T14:08:31.419536Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]}]}